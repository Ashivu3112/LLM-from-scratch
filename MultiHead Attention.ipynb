{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOQRW15+qnhQ0FHWi7PYlzl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"dtciXUd6XZi6","executionInfo":{"status":"ok","timestamp":1764432472041,"user_tz":-330,"elapsed":4263,"user":{"displayName":"Ashish Biswal","userId":"03902015277986563515"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"code","source":["# Define the tensor with 3 rows and 6 columns\n","inputs = torch.tensor(\n","    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n","     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n","     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",")\n","\n","batch = torch.stack((inputs, inputs), dim = 0)\n","batch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"smh1rADOXh0y","executionInfo":{"status":"ok","timestamp":1764435384655,"user_tz":-330,"elapsed":11,"user":{"displayName":"Ashish Biswal","userId":"03902015277986563515"}},"outputId":"52520fae-f2aa-41df-850c-9b3de392a843"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.4300, 0.1500, 0.8900, 0.5500, 0.8700, 0.6600],\n","         [0.5700, 0.8500, 0.6400, 0.2200, 0.5800, 0.3300],\n","         [0.7700, 0.2500, 0.1000, 0.0500, 0.8000, 0.5500]],\n","\n","        [[0.4300, 0.1500, 0.8900, 0.5500, 0.8700, 0.6600],\n","         [0.5700, 0.8500, 0.6400, 0.2200, 0.5800, 0.3300],\n","         [0.7700, 0.2500, 0.1000, 0.0500, 0.8000, 0.5500]]])"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["class GPTConfig():\n","  def __init__(self) -> None:\n","    self.n_heads = 2\n","    self.d_in = 6\n","    self.d_out = 6\n","    self.context_length = 3\n","    self.batch_size = 2\n","    self.device = \"CUDA\" if torch.cuda else \"cpu\""],"metadata":{"id":"1pII96-WX6V-","executionInfo":{"status":"ok","timestamp":1764435476202,"user_tz":-330,"elapsed":44,"user":{"displayName":"Ashish Biswal","userId":"03902015277986563515"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","  def __init__(self, config: GPTConfig) -> None:\n","    super().__init__()\n","    self.config = config\n","    self.W_Q = nn.Linear(self.config.d_in, self.config.d_out, bias = False)\n","    self.W_K = nn.Linear(self.config.d_in, self.config.d_out, bias = False)\n","    self.W_V = nn.Linear(self.config.d_in, self.config.d_out, bias = False)\n","    self.head_dim = self.config.d_out // self.config.n_heads\n","    self.scale = self.head_dim ** -0.5\n","    self.n_heads = self.config.n_heads\n","    self.batch_size = self.config.batch_size\n","    self.register_buffer(\"mask\", torch.tril(torch.ones(self.config.d_out, self.config.d_out)))\n","\n","  def forward(self, x):\n","    context_vector = None\n","    b, n_tokens, d_in = x.shape\n","\n","    print(\"Input shape: \", x.shape, end = \"\\n\")\n","\n","    self.Q = self.W_Q(x)\n","    self.K = self.W_K(x)\n","    self.V = self.W_V(x)\n","\n","    print(\"Q shape: \", self.Q.shape)\n","    print(\"K shape: \", self.K.shape)\n","    print(\"V shape: \", self.V.shape, end = \"\\n\\n\")\n","\n","    # Attention\n","    self.Q = self.Q.view(self.batch_size, n_tokens, self.n_heads, self.head_dim)\n","    self.K = self.K.view(self.batch_size, n_tokens, self.n_heads, self.head_dim)\n","    self.V = self.V.view(self.batch_size, n_tokens, self.n_heads, self.head_dim)\n","\n","    print(\"QKV shape after view\")\n","    print(\"Q shape: \", self.Q.shape)\n","    print(\"K shape: \", self.K.shape)\n","    print(\"V shape: \", self.V.shape, end= \"\\n\\n\")\n","\n","    self.Q = self.Q.transpose(1, 2)\n","    self.K = self.K.transpose(1, 2)\n","    self.V = self.V.transpose(1, 2)\n","\n","    print(\"QKV shape after transpose\")\n","    print(\"Q shape: \", self.Q.shape)\n","    print(\"K shape: \", self.K.shape)\n","    print(\"V shape: \", self.V.shape, end= \"\\n\\n\")\n","\n","    attention_scores = self.Q @ self.K.transpose(-1, -2)\n","    print(\"Attention scores: \" ,attention_scores.shape)\n","    print(attention_scores, end=\"\\n\\n\")\n","\n","    #apply the mask\n","    mask_bool = self.mask.bool()[:n_tokens, :n_tokens]\n","\n","    print(\"mask shape : \", mask_bool.shape)\n","    print(mask_bool, end=\"\\n\\n\")\n","\n","    attention_scores = attention_scores.masked_fill(mask_bool == False, -torch.inf)\n","    print(\"Attention scores after mask: \" ,attention_scores.shape)\n","    print(attention_scores, end=\"\\n\\n\")\n","\n","    attention_weights = torch.softmax(attention_scores * self.config.d_out ** 0.5, dim = -1)\n","    print(\"Attention weights: \", attention_weights.shape)\n","    print(attention_weights, end=\"\\n\\n\")\n","\n","    context_vector = (attention_weights @ self.V).transpose(1, 2)\n","    print(\"Context vector: \", context_vector.shape)\n","    print(context_vector, end=\"\\n\\n\")\n","\n","    context_vector = context_vector.contiguous().view(self.batch_size, self.config.context_length, self.config.d_out)\n","    print(\"Context vector after view: \", context_vector.shape)\n","    print(context_vector, end=\"\\n\\n\")\n","\n","    return context_vector"],"metadata":{"id":"drb7g6vvY3aQ","executionInfo":{"status":"ok","timestamp":1764436462784,"user_tz":-330,"elapsed":21,"user":{"displayName":"Ashish Biswal","userId":"03902015277986563515"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(1332)\n","\n","mha = MultiHeadAttention(GPTConfig())\n","mha(batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Xtrr4dgfxD4","executionInfo":{"status":"ok","timestamp":1764436463677,"user_tz":-330,"elapsed":18,"user":{"displayName":"Ashish Biswal","userId":"03902015277986563515"}},"outputId":"fadbf72c-0cb4-40c8-f54d-02a6aaed0bdf"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape:  torch.Size([2, 3, 6])\n","Q shape:  torch.Size([2, 3, 6])\n","K shape:  torch.Size([2, 3, 6])\n","V shape:  torch.Size([2, 3, 6])\n","\n","QKV shape after view\n","Q shape:  torch.Size([2, 3, 2, 3])\n","K shape:  torch.Size([2, 3, 2, 3])\n","V shape:  torch.Size([2, 3, 2, 3])\n","\n","QKV shape after transpose\n","Q shape:  torch.Size([2, 2, 3, 3])\n","K shape:  torch.Size([2, 2, 3, 3])\n","V shape:  torch.Size([2, 2, 3, 3])\n","\n","Attention scores:  torch.Size([2, 2, 3, 3])\n","tensor([[[[-0.0366, -0.0253,  0.0250],\n","          [ 0.0821, -0.0049, -0.0078],\n","          [-0.0173,  0.0074, -0.0315]],\n","\n","         [[ 0.0909,  0.0849,  0.0057],\n","          [ 0.1518,  0.1120, -0.0596],\n","          [ 0.0210,  0.0333, -0.0547]]],\n","\n","\n","        [[[-0.0366, -0.0253,  0.0250],\n","          [ 0.0821, -0.0049, -0.0078],\n","          [-0.0173,  0.0074, -0.0315]],\n","\n","         [[ 0.0909,  0.0849,  0.0057],\n","          [ 0.1518,  0.1120, -0.0596],\n","          [ 0.0210,  0.0333, -0.0547]]]], grad_fn=<UnsafeViewBackward0>)\n","\n","mask shape :  torch.Size([3, 3])\n","tensor([[ True, False, False],\n","        [ True,  True, False],\n","        [ True,  True,  True]])\n","\n","Attention scores after mask:  torch.Size([2, 2, 3, 3])\n","tensor([[[[-0.0366,    -inf,    -inf],\n","          [ 0.0821, -0.0049,    -inf],\n","          [-0.0173,  0.0074, -0.0315]],\n","\n","         [[ 0.0909,    -inf,    -inf],\n","          [ 0.1518,  0.1120,    -inf],\n","          [ 0.0210,  0.0333, -0.0547]]],\n","\n","\n","        [[[-0.0366,    -inf,    -inf],\n","          [ 0.0821, -0.0049,    -inf],\n","          [-0.0173,  0.0074, -0.0315]],\n","\n","         [[ 0.0909,    -inf,    -inf],\n","          [ 0.1518,  0.1120,    -inf],\n","          [ 0.0210,  0.0333, -0.0547]]]], grad_fn=<MaskedFillBackward0>)\n","\n","Attention weights:  torch.Size([2, 2, 3, 3])\n","tensor([[[[1.0000, 0.0000, 0.0000],\n","          [0.5531, 0.4469, 0.0000],\n","          [0.3302, 0.3509, 0.3189]],\n","\n","         [[1.0000, 0.0000, 0.0000],\n","          [0.5244, 0.4756, 0.0000],\n","          [0.3495, 0.3602, 0.2903]]],\n","\n","\n","        [[[1.0000, 0.0000, 0.0000],\n","          [0.5531, 0.4469, 0.0000],\n","          [0.3302, 0.3509, 0.3189]],\n","\n","         [[1.0000, 0.0000, 0.0000],\n","          [0.5244, 0.4756, 0.0000],\n","          [0.3495, 0.3602, 0.2903]]]], grad_fn=<SoftmaxBackward0>)\n","\n","Context vector:  torch.Size([2, 3, 2, 3])\n","tensor([[[[ 0.3915, -0.3742, -0.0789],\n","          [ 0.1823,  0.0470,  0.4516]],\n","\n","         [[ 0.2928, -0.2979, -0.2070],\n","          [ 0.0497,  0.0251,  0.4967]],\n","\n","         [[ 0.3632, -0.2514, -0.1607],\n","          [ 0.0796, -0.0119,  0.5361]]],\n","\n","\n","        [[[ 0.3915, -0.3742, -0.0789],\n","          [ 0.1823,  0.0470,  0.4516]],\n","\n","         [[ 0.2928, -0.2979, -0.2070],\n","          [ 0.0497,  0.0251,  0.4967]],\n","\n","         [[ 0.3632, -0.2514, -0.1607],\n","          [ 0.0796, -0.0119,  0.5361]]]], grad_fn=<TransposeBackward0>)\n","\n","Context vector after view:  torch.Size([2, 3, 6])\n","tensor([[[ 0.3915, -0.3742, -0.0789,  0.1823,  0.0470,  0.4516],\n","         [ 0.2928, -0.2979, -0.2070,  0.0497,  0.0251,  0.4967],\n","         [ 0.3632, -0.2514, -0.1607,  0.0796, -0.0119,  0.5361]],\n","\n","        [[ 0.3915, -0.3742, -0.0789,  0.1823,  0.0470,  0.4516],\n","         [ 0.2928, -0.2979, -0.2070,  0.0497,  0.0251,  0.4967],\n","         [ 0.3632, -0.2514, -0.1607,  0.0796, -0.0119,  0.5361]]],\n","       grad_fn=<ViewBackward0>)\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0.3915, -0.3742, -0.0789,  0.1823,  0.0470,  0.4516],\n","         [ 0.2928, -0.2979, -0.2070,  0.0497,  0.0251,  0.4967],\n","         [ 0.3632, -0.2514, -0.1607,  0.0796, -0.0119,  0.5361]],\n","\n","        [[ 0.3915, -0.3742, -0.0789,  0.1823,  0.0470,  0.4516],\n","         [ 0.2928, -0.2979, -0.2070,  0.0497,  0.0251,  0.4967],\n","         [ 0.3632, -0.2514, -0.1607,  0.0796, -0.0119,  0.5361]]],\n","       grad_fn=<ViewBackward0>)"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":[],"metadata":{"id":"XaylCNVVf-G1","executionInfo":{"status":"ok","timestamp":1764435711895,"user_tz":-330,"elapsed":12,"user":{"displayName":"Ashish Biswal","userId":"03902015277986563515"}}},"execution_count":40,"outputs":[]}]}