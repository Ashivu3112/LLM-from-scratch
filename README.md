# LLM-from-scratch
Built a large language model from scratch. Implemented tokenization, vector embeddings, multihead attention mechanism to achieve better text generation from the model 

Added below functionalities:
1. Implemented Byte pair encoder to encode both word and character level tokenization.
2. Implemented Simplified attention mechanism with trainable weights
3. Built causal attention mechanism on top of simplified attention mechanism
4. Implemented Multihead Attention mechanism using weights split to optimise training process and reduce computing power.
