{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPUpixH8S4nR+fzI9Fs1ZmZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["1. Causal attention is also known as masked attention mechanism.\n","2. Because we mask out the future tokens. The current token will have the access only uptil the previous tokens. It wont interact with the tokens ahead of it.\n","3. For this we use lower triangular matrix to mask out the future tokens.\n","\n","lower triangular matrix = [[1,0,0,0],\n","                           [1,1,0,0],\n","                           [1,1,1,0],\n","                           [1,1,1,1]]\n","\n","upper triangular matrix = [[1,1,1,1],\n","                           [0,1,1,1],\n","                           [0,0,1,1],\n","                           [0,0,0,1]]\n","\n","Steps to perform causal attention:\n","1. Create input embedding matrix for tokens\n","2. Create Weight_Q, Weight_k, Weight_v matrices\n","3. Generate q, k, v matrices\n","   q = Input @ Weight_q\n","   k = Input @ Weight_k\n","   v = Input @ Weight_v\n","4. Generate attention scores\n","   attention_scores = q @ k.Transpose\n","5. Create a upper triangular matrix mask\n","6. Using the mask as reference check for positions having 1 and fill the corresponding positions in attention_scores matrix with -inf.\n","7. After this attention_scores matrix will look like\n","   attention_scores = [[1, -inf, -inf, -inf],\n","                      [1, 2, -inf, -inf],\n","                      [1, 2, 3, -inf],\n","                      [1, 2, 3, 4]]\n","8. Use softmax for normalisation attention_weights = torch.softmax(attention_scores)\n","9. We can have some dropouts normally 10% for better results\n","10. Generate context vector:\n","    context_vector = attention_weight @ v\n","\n"],"metadata":{"id":"ATkss8LRP7XI"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"IhzGPFv8kb2R","executionInfo":{"status":"ok","timestamp":1763697648934,"user_tz":-330,"elapsed":6586,"user":{"displayName":"Ashish Biswal","userId":"03902015277986563515"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["inputs = torch.tensor(\n","  [[0.43, 0.15, 0.89], # Your     (x^1)\n","   [0.55, 0.87, 0.66], # journey  (x^2)\n","   [0.57, 0.85, 0.64], # starts   (x^3)\n","   [0.22, 0.58, 0.33], # with     (x^4)\n","   [0.77, 0.25, 0.10], # one      (x^5)\n","   [0.05, 0.80, 0.55]] # step     (x^6)\n",")\n","\n","d_in = inputs.shape[1]\n","d_out = 2\n","\n","# The dimensions of Wq, Wk, Wv are (d_in, d_out)\n","# so if shape of inputs is (6, 3)\n","# d_in = 3 => inputs.shape[-1]"],"metadata":{"id":"1vb1aWlelGD6","executionInfo":{"status":"ok","timestamp":1763698014342,"user_tz":-330,"elapsed":25,"user":{"displayName":"Ashish Biswal","userId":"03902015277986563515"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class CausalAttention(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.Wq = nn.Linear(d_in, d_out, bias=False)\n","    self.Wk = nn.Linear(d_in, d_out, bias=False)\n","    self.Wv = nn.Linear(d_in, d_out, bias=False)\n","\n","\n","  def forward(self, x):\n","    self.context_vectors = None\n","\n","    q = self.Wq(x)\n","    k = self.Wk(x)\n","    v = self.Wv(x)\n","\n","    attention_scores = q @ k.transpose(-2, -1)\n","    # k.transpose(-2, -1) = k.transpose( -1, -2)\n","    # swap -1 (last dimension) and -2 (second last dimension)\n","    # This is a generic way of transposing\n","\n","    mask = torch.triu(torch.ones(attention_scores.shape), diagonal=1).bool()\n","\n","    attention_scores.masked_fill_(mask, -float(\"inf\"))\n","    attention_weights = torch.softmax(attention_scores, dim=-1)\n","\n","    self.context_vectors = attention_weights @ v\n","\n","    return self.context_vectors"],"metadata":{"id":"1JajcrDOkjdO","executionInfo":{"status":"ok","timestamp":1763700375505,"user_tz":-330,"elapsed":56,"user":{"displayName":"Ashish Biswal","userId":"03902015277986563515"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["context_vectors = CausalAttention()(inputs)\n","print(context_vectors)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ho6lFWtrNz1","executionInfo":{"status":"ok","timestamp":1763700376281,"user_tz":-330,"elapsed":23,"user":{"displayName":"Ashish Biswal","userId":"03902015277986563515"}},"outputId":"3ba9db66-0fcd-4688-df22-4bdf066d6b90"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.0967, -0.2997],\n","        [-0.1710, -0.2990],\n","        [-0.1895, -0.3022],\n","        [-0.1898, -0.2533],\n","        [-0.1176, -0.2862],\n","        [-0.1728, -0.2327]], grad_fn=<MmBackward0>)\n"]}]},{"cell_type":"code","source":["mask = torch.triu(torch.ones(3,3), diagonal=1).bool()\n","mask"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nSj5xlGUrWHZ","executionInfo":{"status":"ok","timestamp":1763700378489,"user_tz":-330,"elapsed":20,"user":{"displayName":"Ashish Biswal","userId":"03902015277986563515"}},"outputId":"edc18e87-8629-4a85-959e-477321e3fea6"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[False,  True,  True],\n","        [False, False,  True],\n","        [False, False, False]])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":[],"metadata":{"id":"2f5ooqKBvAof"},"execution_count":null,"outputs":[]}]}